{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "âœ… Preprocessed data stored in SQLite successfully!\n"
     ]
    }
   ],
   "source": [
    "import sqlite3\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import ta  # For technical indicators\n",
    "from sklearn.preprocessing import MinMaxScaler\n",
    "\n",
    "# Database path\n",
    "DB_PATH = \"./stocks.db\"\n",
    "TABLE_NAME = \"stock_data\"\n",
    "PROCESSED_TABLE = \"preprocessed_stock_data\"\n",
    "\n",
    "# Connect to SQLite database\n",
    "conn = sqlite3.connect(DB_PATH)\n",
    "\n",
    "# Load the stock data\n",
    "query = f\"SELECT * FROM {TABLE_NAME}\"\n",
    "df = pd.read_sql(query, conn)\n",
    "\n",
    "# Close the connection\n",
    "conn.close()\n",
    "\n",
    "# Convert 'Date' column to datetime format\n",
    "df[\"Date\"] = pd.to_datetime(df[\"Date\"])\n",
    "\n",
    "# Sort data by Date & Ticker for consistency\n",
    "df.sort_values(by=[\"Ticker\", \"Date\"], inplace=True)\n",
    "\n",
    "# Handle missing values using new method\n",
    "df = df.ffill().bfill()\n",
    "\n",
    "# Define function to add technical indicators\n",
    "def add_technical_indicators(df):\n",
    "    # Create rolling moving averages (SMA and EMA)\n",
    "    df[\"SMA_50\"] = df.groupby(\"Ticker\")[\"Close\"].transform(lambda x: x.rolling(window=50, min_periods=1).mean())\n",
    "    df[\"SMA_200\"] = df.groupby(\"Ticker\")[\"Close\"].transform(lambda x: x.rolling(window=200, min_periods=1).mean())\n",
    "    df[\"EMA_50\"] = df.groupby(\"Ticker\")[\"Close\"].transform(lambda x: x.ewm(span=50, adjust=False).mean())\n",
    "    \n",
    "    # RSI (Relative Strength Index)\n",
    "    df[\"RSI\"] = df.groupby(\"Ticker\")[\"Close\"].transform(lambda x: ta.momentum.RSIIndicator(x, window=14).rsi())\n",
    "\n",
    "    # MACD (Moving Average Convergence Divergence)\n",
    "    macd_indicator = ta.trend.MACD(df[\"Close\"], window_slow=26, window_fast=12, window_sign=9)\n",
    "    df[\"MACD\"] = macd_indicator.macd()  # Extract actual MACD values\n",
    "\n",
    "    return df\n",
    "\n",
    "# Add technical indicators\n",
    "df = add_technical_indicators(df)\n",
    "\n",
    "\n",
    "# Normalize numerical columns (excluding categorical ones)\n",
    "numeric_cols = [\"Open\", \"High\", \"Low\", \"Close\", \"Volume\", \"Marketcap\", \"SMA_50\", \"SMA_200\", \"EMA_50\", \"RSI\", \"MACD\"]\n",
    "df[numeric_cols] = df[numeric_cols].astype(np.float32)\n",
    "scaler_dict = {}\n",
    "for ticker, group in df.groupby('Ticker'):\n",
    "    scaler = MinMaxScaler()\n",
    "    scaled_values = scaler.fit_transform(group[numeric_cols])\n",
    "    df.loc[group.index, numeric_cols] = scaled_values\n",
    "    scaler_dict[ticker] = scaler\n",
    "    \n",
    "# Ensure numeric_cols exist in df (some might be missing if they had NaN values)\n",
    "df[numeric_cols] = scaler.fit_transform(df[numeric_cols].fillna(0))\n",
    "\n",
    "# Connect to the database and save the processed data\n",
    "conn = sqlite3.connect(DB_PATH)\n",
    "df.to_sql(PROCESSED_TABLE, conn, if_exists=\"replace\", index=False)\n",
    "conn.close()\n",
    "\n",
    "print(\"âœ… Preprocessed data stored in SQLite successfully!\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Generating 11/871 summaries needed\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Processing Articles: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 11/11 [00:40<00:00,  3.72s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "âœ… Preprocessing complete! Saved to processed_news.csv\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "import requests\n",
    "from tqdm import tqdm\n",
    "\n",
    "# Configuration\n",
    "CSV_PATH = \"nifty50_news.csv\"\n",
    "DEEPSEEK_API_KEY = \"\"\n",
    "API_URL = \"\"\n",
    "\n",
    "def needs_summary(existing_summary):\n",
    "    \"\"\"Check if summary needs regeneration\"\"\"\n",
    "    if pd.isna(existing_summary):\n",
    "        return True\n",
    "    summary = str(existing_summary)\n",
    "    return (\n",
    "        \"summary not available\" in summary.lower() or\n",
    "        \"no description\" in summary.lower() or\n",
    "        summary.strip() in [\"\", \"N/A\", \"nan\"]\n",
    "    )\n",
    "\n",
    "def enhance_summary_with_deepseek(content, existing_summary):\n",
    "    \"\"\"Generate summary only when needed\"\"\"\n",
    "    if not needs_summary(existing_summary):\n",
    "        return existing_summary\n",
    "    \n",
    "    try:\n",
    "        headers = {\n",
    "            \"Authorization\": f\"Bearer {DEEPSEEK_API_KEY}\",\n",
    "            \"Content-Type\": \"application/json\"\n",
    "        }\n",
    "        \n",
    "        data = {\n",
    "            \"model\": \"deepseek/deepseek-chat-v3-0324:free\",\n",
    "            \"messages\": [{\n",
    "                \"role\": \"user\",\n",
    "                \"content\": f\"Create a 2-sentence financial news summary. Content: {str(content)[:1500]}\"\n",
    "            }]\n",
    "        }\n",
    "        \n",
    "        response = requests.post(API_URL, headers=headers, json=data, timeout=15)\n",
    "        \n",
    "        if response.status_code == 200:\n",
    "            return response.json()['choices'][0]['message']['content']\n",
    "            \n",
    "        return existing_summary\n",
    "        \n",
    "    except Exception as e:\n",
    "        print(f\"Error: {str(e)}\")\n",
    "        return existing_summary\n",
    "\n",
    "def preprocess_data():\n",
    "    # Load and clean data\n",
    "    df = pd.read_csv(CSV_PATH)\n",
    "    df = df.dropna(subset=['Title', 'URL'])\n",
    "    df = df.drop_duplicates(subset=['URL'])\n",
    "    \n",
    "    # Process summaries\n",
    "    tqdm.pandas(desc=\"Processing Articles\")\n",
    "    mask = df['Description'].apply(needs_summary)\n",
    "    print(f\"Generating {sum(mask)}/{len(df)} summaries needed\")\n",
    "    \n",
    "    df.loc[mask, 'summary'] = df[mask].progress_apply(\n",
    "        lambda row: enhance_summary_with_deepseek(row['Content'], row['Description']), \n",
    "        axis=1\n",
    "    )\n",
    "    df.loc[~mask, 'summary'] = df.loc[~mask, 'Description']\n",
    "    \n",
    "    # Save processed data to new CSV\n",
    "    df.to_csv(\"processed_news.csv\", index=False)\n",
    "    print(\"âœ… Preprocessing complete! Saved to processed_news.csv\")\n",
    "\n",
    "# Run this cell to preprocess data\n",
    "preprocess_data()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Found 871 existing articles in DB\n",
      "Inserting 0 new articles\n",
      "ðŸ’¡ No new articles to insert\n"
     ]
    }
   ],
   "source": [
    "import sqlite3\n",
    "import pandas as pd\n",
    "from tqdm import tqdm\n",
    "\n",
    "def insert_to_db():\n",
    "    DB_PATH = \"finance_news.db\"\n",
    "    \n",
    "    try:\n",
    "        # Load processed data\n",
    "        df = pd.read_csv(\"processed_news.csv\")\n",
    "        \n",
    "        # Connect to database\n",
    "        conn = sqlite3.connect(DB_PATH)\n",
    "        \n",
    "        # Get existing URLs\n",
    "        existing_urls = pd.read_sql(\"SELECT url FROM nifty_fifty_news\", conn)['url'].tolist()\n",
    "        print(f\"Found {len(existing_urls)} existing articles in DB\")\n",
    "        \n",
    "        # Filter new articles\n",
    "        new_articles = df[~df['URL'].isin(existing_urls)]\n",
    "        print(f\"Inserting {len(new_articles)} new articles\")\n",
    "        \n",
    "        if not new_articles.empty:\n",
    "            # Prepare data for DB\n",
    "            new_articles = new_articles.rename(columns={\n",
    "                'Stock': 'stock',\n",
    "                'Sector': 'sector',\n",
    "                'Title': 'headline',\n",
    "                'Source': 'source',\n",
    "                'Date': 'published_date',\n",
    "                'URL': 'url'\n",
    "            })\n",
    "            \n",
    "            # Insert in chunks with progress\n",
    "            chunks = [new_articles[i:i+100] for i in range(0, len(new_articles), 100)]\n",
    "            with tqdm(total=len(new_articles), desc=\"Inserting to DB\") as pbar:\n",
    "                for chunk in chunks:\n",
    "                    chunk.to_sql('nifty_fifty_news', conn, \n",
    "                                if_exists='append', \n",
    "                                index=False,\n",
    "                                method='multi')\n",
    "                    pbar.update(len(chunk))\n",
    "            print(\"âœ… Database update complete!\")\n",
    "        else:\n",
    "            print(\"ðŸ’¡ No new articles to insert\")\n",
    "            \n",
    "    except Exception as e:\n",
    "        print(f\"ðŸš¨ Error: {str(e)}\")\n",
    "    finally:\n",
    "        if 'conn' in locals():\n",
    "            conn.close()\n",
    "\n",
    "# Run this cell separately to update database\n",
    "insert_to_db()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Device set to use cpu\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Found 871 articles needing sentiment analysis\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Analyzing Sentiment: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 871/871 [02:24<00:00,  6.02it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "âœ… Updated 871 records with sentiment scores\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "import sqlite3\n",
    "import pandas as pd\n",
    "from transformers import AutoTokenizer, AutoModelForSequenceClassification\n",
    "from transformers import pipeline\n",
    "import torch\n",
    "from tqdm import tqdm\n",
    "\n",
    "# Load FinBERT model\n",
    "MODEL_NAME = \"ProsusAI/finbert\"\n",
    "tokenizer = AutoTokenizer.from_pretrained(MODEL_NAME)\n",
    "model = AutoModelForSequenceClassification.from_pretrained(MODEL_NAME)\n",
    "sentiment_analyzer = pipeline(\"text-classification\", model=model, tokenizer=tokenizer)\n",
    "\n",
    "def analyze_sentiment(text):\n",
    "    \"\"\"Get sentiment score using FinBERT\"\"\"\n",
    "    try:\n",
    "        # Truncate text to model's max length (512 tokens)\n",
    "        truncated_text = text[:4000]  # Conservative truncation\n",
    "        result = sentiment_analyzer(truncated_text)[0]\n",
    "        # print(\"score=\", result['score'])\n",
    "        return {\n",
    "            'sentiment': result['label'],\n",
    "            'score': result['score']\n",
    "        }\n",
    "    except Exception as e:\n",
    "        print(f\"Error analyzing sentiment: {str(e)}\")\n",
    "        return {'sentiment': 'neutral', 'score': 0.0}\n",
    "\n",
    "def update_sentiment_scores():\n",
    "    DB_PATH = \"finance_news.db\"\n",
    "    \n",
    "    try:\n",
    "        conn = sqlite3.connect(DB_PATH)\n",
    "        cursor = conn.cursor()\n",
    "        \n",
    "        # Get all articles needing sentiment analysis\n",
    "        df = pd.read_sql(\"\"\"\n",
    "            SELECT id, headline, summary \n",
    "            FROM nifty_fifty_news \n",
    "        \"\"\", conn)\n",
    "        \n",
    "        print(f\"Found {len(df)} articles needing sentiment analysis\")\n",
    "        \n",
    "        # Process in batches\n",
    "        batch_size = 100\n",
    "        updated_count = 0\n",
    "        \n",
    "        with tqdm(total=len(df), desc=\"Analyzing Sentiment\") as pbar:\n",
    "            for i in range(0, len(df), batch_size):\n",
    "                batch = df.iloc[i:i+batch_size]\n",
    "                updates = []\n",
    "                \n",
    "                for _, row in batch.iterrows():\n",
    "                    # Combine headline and summary for better context\n",
    "                    text = f\"{row['headline']}. {row['summary']}\"\n",
    "                    sentiment = analyze_sentiment(text)\n",
    "                    \n",
    "                    updates.append((\n",
    "                        sentiment['sentiment'],\n",
    "                        sentiment['score'],\n",
    "                        row['id']\n",
    "                    ))\n",
    "                    pbar.update(1)\n",
    "                \n",
    "                # Batch update\n",
    "                cursor.executemany(\"\"\"\n",
    "                    UPDATE nifty_fifty_news\n",
    "                    SET sentiment = ?, sentiment_score = ?\n",
    "                    WHERE id = ?\n",
    "                \"\"\", updates)\n",
    "                conn.commit()\n",
    "                updated_count += len(updates)\n",
    "        \n",
    "        print(f\"âœ… Updated {updated_count} records with sentiment scores\")\n",
    "        \n",
    "    except Exception as e:\n",
    "        print(f\"ðŸš¨ Error: {str(e)}\")\n",
    "    finally:\n",
    "        if conn:\n",
    "            conn.close()\n",
    "\n",
    "# Run the sentiment analysis\n",
    "update_sentiment_scores()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Collecting transformers\n",
      "  Using cached transformers-4.50.2-py3-none-any.whl.metadata (39 kB)\n",
      "Collecting torch\n",
      "  Using cached torch-2.6.0-cp311-cp311-win_amd64.whl.metadata (28 kB)\n",
      "Collecting sentencepiece\n",
      "  Downloading sentencepiece-0.2.0-cp311-cp311-win_amd64.whl.metadata (8.3 kB)\n",
      "Collecting filelock (from transformers)\n",
      "  Using cached filelock-3.18.0-py3-none-any.whl.metadata (2.9 kB)\n",
      "Collecting huggingface-hub<1.0,>=0.26.0 (from transformers)\n",
      "  Using cached huggingface_hub-0.29.3-py3-none-any.whl.metadata (13 kB)\n",
      "Requirement already satisfied: numpy>=1.17 in c:\\users\\apara\\appdata\\local\\programs\\python\\python311\\lib\\site-packages (from transformers) (1.24.3)\n",
      "Requirement already satisfied: packaging>=20.0 in c:\\users\\apara\\appdata\\local\\programs\\python\\python311\\lib\\site-packages (from transformers) (24.2)\n",
      "Requirement already satisfied: pyyaml>=5.1 in c:\\users\\apara\\appdata\\local\\programs\\python\\python311\\lib\\site-packages (from transformers) (6.0.2)\n",
      "Collecting regex!=2019.12.17 (from transformers)\n",
      "  Using cached regex-2024.11.6-cp311-cp311-win_amd64.whl.metadata (41 kB)\n",
      "Requirement already satisfied: requests in c:\\users\\apara\\appdata\\local\\programs\\python\\python311\\lib\\site-packages (from transformers) (2.32.3)\n",
      "Collecting tokenizers<0.22,>=0.21 (from transformers)\n",
      "  Using cached tokenizers-0.21.1-cp39-abi3-win_amd64.whl.metadata (6.9 kB)\n",
      "Collecting safetensors>=0.4.3 (from transformers)\n",
      "  Using cached safetensors-0.5.3-cp38-abi3-win_amd64.whl.metadata (3.9 kB)\n",
      "Requirement already satisfied: tqdm>=4.27 in c:\\users\\apara\\appdata\\local\\programs\\python\\python311\\lib\\site-packages (from transformers) (4.67.1)\n",
      "Requirement already satisfied: typing-extensions>=4.10.0 in c:\\users\\apara\\appdata\\local\\programs\\python\\python311\\lib\\site-packages (from torch) (4.13.0)\n",
      "Collecting networkx (from torch)\n",
      "  Using cached networkx-3.4.2-py3-none-any.whl.metadata (6.3 kB)\n",
      "Requirement already satisfied: jinja2 in c:\\users\\apara\\appdata\\local\\programs\\python\\python311\\lib\\site-packages (from torch) (3.1.6)\n",
      "Collecting fsspec (from torch)\n",
      "  Using cached fsspec-2025.3.0-py3-none-any.whl.metadata (11 kB)\n",
      "Collecting sympy==1.13.1 (from torch)\n",
      "  Using cached sympy-1.13.1-py3-none-any.whl.metadata (12 kB)\n",
      "Collecting mpmath<1.4,>=1.1.0 (from sympy==1.13.1->torch)\n",
      "  Using cached mpmath-1.3.0-py3-none-any.whl.metadata (8.6 kB)\n",
      "Requirement already satisfied: colorama in c:\\users\\apara\\appdata\\local\\programs\\python\\python311\\lib\\site-packages (from tqdm>=4.27->transformers) (0.4.6)\n",
      "Requirement already satisfied: MarkupSafe>=2.0 in c:\\users\\apara\\appdata\\local\\programs\\python\\python311\\lib\\site-packages (from jinja2->torch) (3.0.2)\n",
      "Requirement already satisfied: charset-normalizer<4,>=2 in c:\\users\\apara\\appdata\\local\\programs\\python\\python311\\lib\\site-packages (from requests->transformers) (3.4.1)\n",
      "Requirement already satisfied: idna<4,>=2.5 in c:\\users\\apara\\appdata\\local\\programs\\python\\python311\\lib\\site-packages (from requests->transformers) (3.10)\n",
      "Requirement already satisfied: urllib3<3,>=1.21.1 in c:\\users\\apara\\appdata\\local\\programs\\python\\python311\\lib\\site-packages (from requests->transformers) (2.3.0)\n",
      "Requirement already satisfied: certifi>=2017.4.17 in c:\\users\\apara\\appdata\\local\\programs\\python\\python311\\lib\\site-packages (from requests->transformers) (2025.1.31)\n",
      "Using cached transformers-4.50.2-py3-none-any.whl (10.2 MB)\n",
      "Using cached torch-2.6.0-cp311-cp311-win_amd64.whl (204.2 MB)\n",
      "Using cached sympy-1.13.1-py3-none-any.whl (6.2 MB)\n",
      "Downloading sentencepiece-0.2.0-cp311-cp311-win_amd64.whl (991 kB)\n",
      "   ---------------------------------------- 0.0/991.5 kB ? eta -:--:--\n",
      "   ---------------------------------------- 0.0/991.5 kB ? eta -:--:--\n",
      "   ---------- ----------------------------- 262.1/991.5 kB ? eta -:--:--\n",
      "   ------------------------------- -------- 786.4/991.5 kB 1.3 MB/s eta 0:00:01\n",
      "   ---------------------------------------- 991.5/991.5 kB 1.3 MB/s eta 0:00:00\n",
      "Using cached huggingface_hub-0.29.3-py3-none-any.whl (468 kB)\n",
      "Using cached fsspec-2025.3.0-py3-none-any.whl (193 kB)\n",
      "Using cached regex-2024.11.6-cp311-cp311-win_amd64.whl (274 kB)\n",
      "Using cached safetensors-0.5.3-cp38-abi3-win_amd64.whl (308 kB)\n",
      "Using cached tokenizers-0.21.1-cp39-abi3-win_amd64.whl (2.4 MB)\n",
      "Using cached filelock-3.18.0-py3-none-any.whl (16 kB)\n",
      "Using cached networkx-3.4.2-py3-none-any.whl (1.7 MB)\n",
      "Using cached mpmath-1.3.0-py3-none-any.whl (536 kB)\n",
      "Installing collected packages: sentencepiece, mpmath, sympy, safetensors, regex, networkx, fsspec, filelock, torch, huggingface-hub, tokenizers, transformers\n",
      "Successfully installed filelock-3.18.0 fsspec-2025.3.0 huggingface-hub-0.29.3 mpmath-1.3.0 networkx-3.4.2 regex-2024.11.6 safetensors-0.5.3 sentencepiece-0.2.0 sympy-1.13.1 tokenizers-0.21.1 torch-2.6.0 transformers-4.50.2\n"
     ]
    }
   ],
   "source": [
    "!pip install transformers torch sentencepiece"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
